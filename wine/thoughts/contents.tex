\chapter{Defining the problem}

\section{Predict the cultivar of wine}
We have data from wines from three different cultivars from the same region in Italy \cite{info}. Given some qualities from the wine, like alchohol, acid, etc. we have to predict which cultivar derived an instance.

\section{History of the dataset}
The dataset was dontated by Riccardo Leardi, the data is from three different cultivars from the same region in Italy \cite{info}.

\section{Attribute information}
All attributes are continuous
\begin{enumerate}
    \item Alchohol 
    \item Malic acid
    \item Ash
    \item Alcalinity of ash
    \item Magnesium
    \item Total phenols 
    \item Flavanoids
    \item Nonflavanoid phenols
    \item Proanthocyanins 
    \item Color intensity 
    \item Hue 
    \item OD280/OD315 of diluted wines 
    \item Proline
\end{enumerate}

\section{First look at the dataset}
There are a lot of features that seem to separate the features quite well. For this reason, discriminant analysis looks very promising, since it finds a linear combination of features that best suits the problem. The baseline accuracy is 40\%, since that is what we get when we always predict the major class.

\chapter{Analyze the data}
\section{Handling missing values}
There are no missing values in the dataset
\section{Normalization and standardization}
We may benefit from standardization, since some attributes have a Gaussian-like distribution.

\chapter{Prepare data}
To analyze the dataset, we will use the following views of the dataset:
\section{Raw view of the dataset}
The dataset in the same view as we downloaded it.
\section{Standardized view of the dataset}
The dataset after applying the standardize filter

\chapter{Evaluate algorithms}
\section{Initial experiment design}
We will run the experiment design on the the two datasets descibed in the previous chapter (raw and standardized) on the following algorithms:
\begin{enumerate}
    \item Decision tree - C4
    \item Naive Bayes
    \item Logistic regression
    \item Linear discriminant analysis
    \item Quadratic discriminant analysis
    \item Support vector machine using sequantial minimal optimization
    \item Multilayer perceptron 
    \item Nearest neighbour
\end{enumerate}
The results show 99,16\% accuracy for Quadratic Discriminant Analysis. Since the results are on 10-fold cross validation, we are not overfitting and the results confirm our intuition that the classes are clearly separable through a combination of the attributes.

\chapter{Present results}