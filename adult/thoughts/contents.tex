\chapter{Defining the problem}

\section{Predict whether salary exceeds 50,000}
Prediction task is to determine whether a person makes over 50K a year by given data for his social background. 

\section{History of the dataset}
Extraction was done by Barry Becker from the 1994 Census database
The dataset has 48,882 instances, 14 attributes, has missing values and our problem is a class

\section{Attribute information}
The dataset contains the following attributes about a person ~\cite{uci-link}:
\begin{enumerate}
    \item Age - continuos
    \item Workclass - Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.
    \item Fnlwgt - sample weighting; this should be ignored while creating models
    \item Education - Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool. 
    \item Education-num: continuous (how many years has one learned)
    \item Marital status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse. 
    \item Occupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.
    \item Relationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried. 
    \item Race: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black. 
    \item Sex: Female, Male. 
    \item Capital gain: continuous, how much an individual has learned outside his usual wage
    \item Capital loss: continuous, how much an individual has lost outside his usual wage.
    \item Hours per week: continuous, how many hours per week an individual works
    \item Native country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad and Tobago, Peru, Hong, Holand-Netherlands.
\end{enumerate}
\section{First look at the dataset}
There are a lot more examples from the first class (<50k earnings). If we always predict the majority class, we get accuracy of 76.07\%. This is a baseline accuracy, we obviously need result better than that. The accuracy metric is too biased and we will use the Kappa metric, since the classes are unbalanced. There are features of different scales. Two attributes have missing values: work class(6\%) and native country(2\%). There are no obvious correlation between the features and the output class \ref{fig:visual}.
\begin{figure}
    \centering
    \includegraphics[scale=0.3]{visualize_all}
    \caption{Attributes correlation with output class visualized}
    \label{fig:visual}
\end{figure}

\chapter{Analyze the data}
Since there are 48842 instances, we will use hold out set. We will split the dataset and we will use only 75\% of it. We will use the other 25\% at the end, to test how our model generalizes. Since the attributes have a lot of different scales, we will almost certainly benefit from data transformations like normalization and standardization. There does not seem to be attributes with Gaussian-like distributions, so methods which assume normal distribution like Logistic Regression and Linear Discriminant Analysis. We have a lot of attributes, so we should consider removing some of them to remove the noise and speed up the model. Since there are a lot of points, k-NN will be very slow if we search every datapoint, so alternative methods should be used like KD-trees or locality sensitivity hashing. Also note that if features are on the same scale, Euclidean distance is fine, but if they are on different, Manhattan distance should be preferred.
\section{Handling missing values}
For this dataset, it looks like the most logical decision is to replace the missing values with the mean or median for every attribute, since imputing the rows will likely lose data in this noisy dataset.
\section{Normalization and standardization}
Standardization does not seem like a good solution, since the attributes do not look Gaussian distributed. A normalized dataset however makes sense, and one is available in the Github repository for the project \cite{repo}.

\chapter{Prepare data}
We will now present different views of the datasets. Note that in every view, the attribute Fnlwgt was removed.
\section{Raw view of the dataset}
The dataset as it was downloaded from UCI Machine Learning Repository \cite{uci-link}, with Fnlwgt removed.
\section{Normalized view of the dataset}
The dataset as it was downloaded from UCI Machine Learning Repository \cite{uci-link}, with Fnlwgt removed, after applying normalizing filter.
\section{Dataset after feature selection}
In the normalized view, after evaluting attributes by their info gain and selecting the top 8, we get the following attributes: \ref{fig:attr}.
\begin{figure}
    \centering
    \includegraphics[scale=1]{selected_attibutes}
    \caption{Selected attributes from feature selection}
    \label{fig:attr}
\end{figure}

\chapter{Evaluate algorithms}
\section{Initial experiment design}
We will evaluate the datasets described in the previous chapter, and for algorithms we will use:
\begin{enumerate}
    \item Naive Bayes
    \item Logistic regression with ridge parameter $10^{-8}$
    \item Pruned C4 decision tree with confidence factor 0.25 and 2 minimum objects
    \item Linear Discriminant Analysis classifier
    \item k-Nearest Neighbor with 3 neighbors and KD-tree for searching algorithm
    \item Support vector machine using sequential minimal optimization for training and logistic regression for calibrator with radial basis function kernel and 0.001 tolerance parameter.
    \item Random forest
    \item Bootstraping aggregation
    \item Adaboost with decision stump classifier run on 100 iterations
    \item Stacking of C4 decision trees
\end{enumerate}
After the test run we see \cite{repo} that the stacking of C4 decision trees and the C4 decision tree alone give the highest results, and we will of course choose the C4 decision tree to improve the results in the next section.

\chapter{Impove results}
\section{Experiment for improving result}
We will now run the experiment with different confidence scores of the C4 decision tree:
\begin{enumerate}
    \item Confidence score: 0.1
    \item Confidence score: 0.25
    \item Confidence score: 0.4
    \item Confidence score: 0.5
\end{enumerate}
As we can see from the results \cite{repo}, the optimal decision tree is the one with confidence score 0.5 on the dataset which is not normalized at all. It achieves 0.60 Kappa statistic and accuracy of nearly 86\%.

\chapter{Present results}
The final model, C4 decision tree with confidence score 0.5, achieves 85.595\% accuracy on the unseen part of the data. See the Github repository \cite{repo} for more info.